---
title: "R Notebook"
output: html_notebook
---

#Read classic and DL features
```{r}

# classic <- read.csv(file = "Features/all_cell_data.csv")
# deeplearn <- read.csv(file = "Features/dgcnn_foldingnet_128_001_cell.csv")
deeplearn <- read.csv(file = "Features/dgcnn_foldingnet_128_001_cell_updated.csv")

dim(classic)
dim(deeplearn)
```
```{r}
library(pls)
data(yarn)
data(oliveoil)
data(gasoline)

head(gasoline)

gasTrain <- gasoline[1:50,]
gasTest <- gasoline[51:60,]

gasTrain

gas1 <- plsr(octane ~ NIR, ncomp = 10, data = gasTrain, validation = "LOO")

summary(gas1)


plot(RMSEP(gas1), legendpos = "topright")

plot(gas1, ncomp = 2, asp = 1, line = TRUE)

plot(gas1, plottype = "scores", comps = 1:3)

explvar(gas1)

plot(gas1, "loadings", comps = 1:2, legendpos = "topleft",
labels = "numbers", xlab = "nm")
abline(h = 0)

predict(gas1, ncomp = 2, newdata = gasTest)

RMSEP(gas1, newdata = gasTest)

```

21-03-18_17-38-12
21-03-15_12-37-27
21-03-18_17-38-12
#setup data for PLSR
```{r}
colnames(deeplearn)

deeplearn$plateLabel <- ifelse(grepl(pattern = "21-03-18_17-38-12",deeplearn$serialNumber), "Plate_A",
             ifelse(grepl(pattern = "21-03-15_12-37-27", deeplearn$serialNumber), "Plate_B",
                          "Plate_C"))


#Make variable names as numbers for inspection

colnames(deeplearn) <- gsub("X","",colnames(deeplearn))


#remove variable that is just rownames

deeplearn <- deeplearn[-1]



levels(as.factor(deeplearn$Treatment))

library(dplyr)
cont_dl_all <- deeplearn %>%
  filter(Treatment == "DMSO" |
          Treatment == "No Treatment")

#remove erk outliers
hist(cont_dl_all$erkRatio,100)

cont_dl_all <- cont_dl_all %>%
  filter(erkRatio < 5)

dim(cont_dl_all)



#select only AI and erkRatio (predictor)
colnames(cont_dl_all)
cont_dl <- cont_dl_all[,c(1:129, 133)]

#shuffle the data
set.seed(1)
shuffled_fdl <-  cont_dl[sample(1:nrow(cont_dl)), ]


colnames(shuffled_fdl)

#get training dataset

aboutHalf <- ceiling((nrow(shuffled_fdl)/2))

fdl_train <- shuffled_fdl[1:aboutHalf,]
fdl_test <- shuffled_fdl[aboutHalf:nrow(shuffled_fdl),]

fdl_train

colnames(fdl_train)
```
```{r}




```

#schedule notifications

mailR
https://towardsdatascience.com/effective-notification-mechanisms-in-r-82db9cb8816
14:33
```{r}


print(paste("started at:",Sys.time()))
start <- Sys.time()
erk1 <- plsr(erkRatio ~ ., ncomp = 2, data = fdl_train, validation = "LOO")
end <- Sys.time()
print(paste("Ended at:",Sys.time()))
duration <- end - start
duration
```

#second model
```{r}
# print(paste("started at:",Sys.time()))
# start <- Sys.time()
# erk2 <- plsr(erkRatio ~ ., ncomp = 40, data = fdl_train, validation = "LOO")
# end <- Sys.time()
# print(paste("Ended at:",Sys.time()))
# duration <- end - start
# duration
```
#stopped at 16:48 after running from 15:23



#Predictions TODO with PLSR
1. predict erk ratio
2. find most important features for erkRatio
3. use PCs to predict erk ratio
4. find most important PCS for erk ratio
5. translate what are the high loaded erk ratio predictors
6. find cells that have high or low erk ratio
7. find cells at intervals along the erk ratio prediction gradient
eg. for low ratio, 0 ratio and high ratio
--> get the 9 cells that are closest to the 



```{r}
summary(erk1)

str(erk1$loadings)

# copyErk1 <- erk1


plot(RMSEP(erk1), legendpos = "topright")

plot(erk1, ncomp = 20, asp = 1, line = TRUE)



plot(erk1, plottype = "scores", comps = 1:3)

explvar(erk1)

plot(erk1, "loadings", comps = 1:2, legendpos = "topleft",
labels = "numbers", xlab = "nm")
abline(h = 0)

predict(erk1, ncomp = 2, newdata = fdl_test)

plot(RMSEP(erk1, newdata = fdl_test))

```



```{r}
# head(cont_dl)
# head(shuffled_fdl)
# 
# #plates labels
# 
# # deeplearn$serialNumber
# #get just deeplearn features (fdl)
# 
# fdl <- deeplearn[,c(2:129, 134)]
# 
# head(fdl)
# 



# clusters for new clustering, nocodazole, the cluster that it's in isnt the small round cells

```



```{r}
library(pls)
PLSR_model <- plsr(lts_response ~ ., data=lts_predictors, scale=TRUE, validation="CV")

```


#View data
70 classic features
136 deep learn features
```{r}

head(classic)
colnames(deeplearn)
head(deeplearn)

deeplearn$serialNumber
```

# Join the two datasets
```{r}

mergeFeatures <- dplyr::left_join(classic, deeplearn, by = "serialNumber")

dim(mergeFeatures)

colnames(mergeFeatures)
head(mergeFeatures)
library(dplyr)
subMerge <- mergeFeatures[,c(10:62,72:199)]

cor_subMerge <- cor(subMerge, use = "complete.obs")

```

#Correlation matrix
```{r}

dim(cor_subMerge)

CLvDL <- cor_subMerge[1:53,54:181]

cellPitchAzumith <- cor_subMerge[20:21,54:181]


rownames(cor_subMerge)

```

#Visualise
```{r}
library(ComplexHeatmap)
Heatmap(CLvDL)
Heatmap(cellPitchAzumith)

str(cellPitchAzumith)

hist(cellPitchAzumith[1,], 20) #Azumith
hist(cellPitchAzumith[2,], 20) #Pitch
```


#get correlation between every X0-Xn deep learning feature and classical feature
```{r}
cor(x, method = c("pearson", "kendall", "spearman"))
cor(my_data, use = "complete.obs")
rcorr(x, type = c("pearson","spearman"))

```

#remove features with correlation greater
```{r}



```

